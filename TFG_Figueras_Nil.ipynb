{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importació de llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import graphviz\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Reshape, Flatten, Conv1D, Conv1DTranspose, LeakyReLU, Concatenate\n",
    "from keras.optimizers.legacy import Adam, Adamax\n",
    "from numpy import expand_dims, ones, zeros, vstack\n",
    "from numpy.random import randn, randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.layers import Input, RepeatVector, TimeDistributed, Permute\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Càrrega de dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afegir al path\n",
    "# Add 'C:\\Program Files\\Graphviz\\bin' to the system path\n",
    "os.environ['PATH'] += os.pathsep + 'C:\\\\Program Files\\\\Graphviz\\\\bin\\\\*'\n",
    "# Get the cwd\n",
    "cwd = os.getcwd()\n",
    "cwd_parent = os.path.dirname(cwd)\n",
    "# Get the paths to the data and src folders\n",
    "data_path = os.path.join(cwd_parent, 'data')\n",
    "src_path = os.path.join(cwd_parent, 'src')\n",
    "\n",
    "# Get the cwd\n",
    "cwd = os.getcwd()\n",
    "cwd_parent = os.path.dirname(cwd)\n",
    "# Get the paths to the data and src folders\n",
    "data_path = os.path.join(cwd_parent, 'data')\n",
    "\n",
    "# Defineix el rang de dates desitjat per al 2023\n",
    "start_date = \"2005-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "start_year = int(start_date[:4])\n",
    "end_year = int(end_date[:4])\n",
    "company = 'MMM'\n",
    "\n",
    "\n",
    "# Descarrega les dades amb intervals d'1 hora\n",
    "s_and_p_data = yf.download(company, start=start_date, end=end_date, interval='1d')\n",
    "# Convertir la columna de dates a tipus datetime\n",
    "s_and_p_data.index = pd.to_datetime(s_and_p_data.index)\n",
    "# Fer una còpia del conjunt de dades amb les capçaleres correctes\n",
    "s_and_p_data.reset_index(inplace=True)\n",
    "#s_and_p_data['Date'] = (s_and_p_data['Date'] - pd.Timestamp(start_date)) // pd.Timedelta('1d')\n",
    "data_features = s_and_p_data.copy()\n",
    "# Obtenció del dia de la setmana a partir de la data\n",
    "data_features['Weekday'] = data_features['Date'].dt.dayofweek\n",
    "data_features['Company'] = company\n",
    "data_features.to_csv(os.path.join(data_path, f'{company}_1d_{start_year}_to_{end_year}.csv'))\n",
    "\n",
    "# Load the csv file \n",
    "data = pd.read_csv(os.path.join(data_path, f'{company}_1d_{start_year}_to_{end_year}.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apliquem una rolling window amb numpy a les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.array(data_features[['Close', 'Volume', 'Weekday']])\n",
    "\n",
    "window_len = 20\n",
    "\n",
    "# Escalar la columna 'Volume'\n",
    "scaler = MinMaxScaler()\n",
    "data_array[:, 1] = scaler.fit_transform(data_array[:, 1].reshape(-1, 1)).flatten()\n",
    "\n",
    "x_array = np.zeros((len(data_array)-window_len, window_len, data_array.shape[1]))\n",
    "y_array = np.zeros((len(data_array)-window_len))\n",
    "for row in range(data_array.shape[0] - window_len - 1):\n",
    "    x_array[row] = data_array[row:row + window_len]\n",
    "    y_array[row] = data_array[row + window_len][0]\n",
    "\n",
    "print(data_array)\n",
    "# Comprovem que és correcte\n",
    "print(y_array[0] == x_array[1, -1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separem entre entrenament i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporcions de les seccions\n",
    "sections = [(0.15, 'train'), (0.05, 'test'), (0.15, 'train'), (0.05, 'test'), \n",
    "            (0.15, 'train'), (0.05, 'test'), (0.15, 'train'), (0.05, 'test'), \n",
    "            (0.15, 'train')]\n",
    "\n",
    "def split_data_into_sections(x_array, y_array, sections):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sections based on the provided proportions.\n",
    "\n",
    "    Parameters:\n",
    "    - x_array: The array of features.\n",
    "    - y_array: The array of labels.\n",
    "    - sections: A list of tuples where each tuple contains a proportion and a label ('train' or 'test').\n",
    "\n",
    "    Returns:\n",
    "    - x_train: List of training features.\n",
    "    - y_train: List of training labels.\n",
    "    - x_test: List of testing features.\n",
    "    - y_test: List of testing labels.\n",
    "    \"\"\"\n",
    "    x_train, y_train = [], []\n",
    "    x_test, y_test = [], []\n",
    "    total_rows = len(x_array)\n",
    "    current_index = 0\n",
    "\n",
    "    for proportion, label in sections:\n",
    "        section_size = int(proportion * total_rows)\n",
    "        if label == 'train':\n",
    "            x_train.extend(x_array[current_index:current_index + section_size])\n",
    "            y_train.extend(y_array[current_index:current_index + section_size])\n",
    "        elif label == 'test':\n",
    "            x_test.extend(x_array[current_index:current_index + section_size])\n",
    "            y_test.extend(y_array[current_index:current_index + section_size])\n",
    "        current_index += section_size\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data_into_sections(x_array, y_array, sections)\n",
    "\n",
    "# Convertir llistes a arrays numpy per ser consistents amb l'entrada\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim funcions de la C-GAN i la pròpia C-GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\n",
    "    # Generate n_samples points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # Reshape the points into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # Return the generated points\n",
    "    return x_input\n",
    "\n",
    "# El discriminador prediu real o fals\n",
    "# y_discriminador = 1/0\n",
    "# labels_discriminador = el que condiciona al que generem\n",
    "# X_discriminador = El que estem generant en si\n",
    "\n",
    "def generate_real_samples(x_data, y_data, n_samples):\n",
    "    # Deteminem n_samples a l'atzar\n",
    "    ix = randint(0, x_data.shape[0], n_samples)\n",
    "    # Ens quedem amb els valors triats a l'atzar\n",
    "    x_disc, labels_disc = x_data[ix], y_data[ix]\n",
    "    # Generem y_disc amb 1, que vol dir reals\n",
    "    y_disc = ones((n_samples, 1))\n",
    "    return [x_disc, labels_disc], y_disc\n",
    "\n",
    "\n",
    "def define_discriminator(n_labels, win_len, opt, loss):\n",
    "    # label input\n",
    "    in_feat = Input(shape=(win_len, n_labels))\n",
    "    in_feat_1 = LSTM(win_len * n_labels, return_sequences=True)(in_feat)\n",
    "    in_feat_2 = LSTM(win_len * n_labels, return_sequences=False)(in_feat_1)\n",
    "    in_feat_3 = Dense(win_len * n_labels)(in_feat_2)\n",
    "    in_feat_4 = Reshape((win_len, n_labels))(in_feat_3)\n",
    "\n",
    "    # generator output\n",
    "    x_disc = Input(shape=(1,))\n",
    "    x_disc_1 = Dense(n_labels * win_len)(x_disc)\n",
    "    x_disc_2 = Reshape((win_len, n_labels))(x_disc_1)\n",
    "\n",
    "    # merge generator output and label input\n",
    "    merge = Concatenate()([in_feat_4, x_disc_2])\n",
    "\n",
    "    # output layer\n",
    "    out_layer = Dense(1, activation='sigmoid')(merge)\n",
    "\n",
    "    # define model\n",
    "    model = Model([in_feat, x_disc], out_layer)\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_generator(latent_dim, n_labels, win_len):\n",
    "    # label input\n",
    "    in_feat = Input(shape=(win_len, n_labels))\n",
    "    in_feat_1 = LSTM(win_len * n_labels, return_sequences=True)(in_feat)\n",
    "    in_feat_2 = LSTM(latent_dim * win_len, return_sequences=False)(in_feat_1)\n",
    "    # reshape to the latent_dim\n",
    "    in_feat_3 = Reshape((latent_dim, win_len))(in_feat_2)\n",
    "    \n",
    "    # latent_dim generator input\n",
    "    in_lat = Input(shape=(latent_dim, 1))\n",
    "    in_lat_1 = LSTM(latent_dim, return_sequences=False)(in_lat)\n",
    "    in_lat_2 = LeakyReLU(alpha=0.2)(in_lat_1)\n",
    "    in_lat_3 = Reshape((latent_dim, 1))(in_lat_2)\n",
    "\n",
    "    # merge RA gen and label input\n",
    "    merge = Concatenate()([in_lat_3, in_feat_3])\n",
    "    gen = LSTM(50, return_sequences=True)(merge)\n",
    "    gen_1 = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen_2 = LSTM(100, return_sequences=False)(gen_1)\n",
    "    # output is a dense with ReLU activation to ensure non-negative output\n",
    "    out_layer = Dense(1, activation='relu')(gen_2)\n",
    "    # define model\n",
    "    model = Model([in_feat, in_lat], out_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define the GAN model\n",
    "def define_gan(generator, discriminator, opt, loss, latent_dim, win_len, n_labels):\n",
    "    discriminator.trainable = False\n",
    "    noise_input = Input(shape=(latent_dim,))\n",
    "    label_input = Input(shape=(win_len, n_labels))\n",
    "    generated_sequence = generator([label_input, noise_input])\n",
    "    validity = discriminator([label_input, generated_sequence])\n",
    "    model = Model([label_input, noise_input], validity)\n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#generate latent points\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # Generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # Reshape the points into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    # Return the generated points\n",
    "    return x_input\n",
    "\n",
    "# Generate fake samples with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples, y_data):\n",
    "    # Generate points in latent space\n",
    "    latent_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # Generate GAN input\n",
    "    X = generator.predict([y_data, latent_input], verbose=0)\n",
    "    # Create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [X, y_data], y\n",
    "\n",
    "# Generate real samples\n",
    "def generate_real_samples(x_data, y_data, n_samples):\n",
    "    # Deteminem n_samples a l'atzar\n",
    "    ix = randint(0, x_data.shape[0], n_samples)\n",
    "    # Ens quedem amb els valors triats a l'atzar\n",
    "    labels, x_real = x_data[ix], y_data[ix]\n",
    "    # Generem y_disc amb 1, que vol dir reals\n",
    "    y_real = ones((n_samples, 1))\n",
    "    return [labels, x_real], y_real\n",
    "\n",
    "# Define a printing functions to plot all the x_train values, y_real and y_fake\n",
    "def plot_all(labels, real_value, prediction):\n",
    "    # Plot in 3 subplots\n",
    "    fig, axs = plt.subplots(3)\n",
    "    # Plot the real values\n",
    "    axs[0].plot(labels[:,0], label='Close')\n",
    "    axs[1].plot(labels[:,1], label='Volume')\n",
    "    axs[2].plot(labels[:,2], label='Weekday')\n",
    "    plt.show()\n",
    "\n",
    "# Train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, x_train, y_train, latent_dim, exp_name, company, n_epochs=10000, n_batch=128, save_ratio=25, last_epoch=0):\n",
    "    if not os.path.exists(os.path.join(cwd, company, exp_name, 'images')):\n",
    "        os.makedirs(os.path.join(cwd, company, exp_name, 'images'))\n",
    "    bat_per_epo = int(x_train.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_loss_1_hist = []\n",
    "    d_loss_2_hist = []\n",
    "    g_loss_hist = []\n",
    "    # manually enumerate epochs\n",
    "    for i in range(last_epoch, n_epochs):\n",
    "        # Clear cell output\n",
    "        print(f'Epoch {i}/{n_epochs}')\n",
    "        for j in range(bat_per_epo):\n",
    "            print(f'Epoch: {i}/{n_epochs}, Batch: {j}/{bat_per_epo}')\n",
    "            clear_output(wait=True)\n",
    "            # Get randomly selected 'real' samples\n",
    "            [labels_real, x_real], y_real = generate_real_samples(x_train, y_train, half_batch)\n",
    "            # Update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch([labels_real, x_real], y_real)\n",
    "            d_loss_1_hist.append(d_loss1)\n",
    "            # Generate 'fake' examples\n",
    "            [x_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch, labels_real)\n",
    "            # Reshape x_fake to have the same shape as x_real\n",
    "            x_fake = x_fake.reshape(x_real.shape)\n",
    "            # Update discriminator model weights\n",
    "            d_loss2, _ = d_model.train_on_batch([labels_real, x_fake], y_fake)\n",
    "            d_loss_2_hist.append(d_loss2)\n",
    "            # Prepare points in latent space as input for the generator\n",
    "            x_gan = generate_latent_points(latent_dim, half_batch)\n",
    "            # Create inverted labels for the fake samples\n",
    "            y_gan = ones((half_batch, 1))\n",
    "            # Update the generator via the discriminator's error\n",
    "            g_loss, _ = gan_model.train_on_batch([labels_real, x_gan], y_gan)\n",
    "            g_loss_hist.append(g_loss)\n",
    "            # plot the loss on each iteration\n",
    "            #print(f'Epoch {i}/{n_epochs}, Batch {j}/{bat_per_epo}, d1={d_loss1}, d2={d_loss2}, g={g_loss}')\n",
    "            plt.plot(d_loss_1_hist, label='discriminator real')\n",
    "            plt.plot(d_loss_2_hist, label='discriminator fake')\n",
    "            plt.plot(g_loss_hist, label='generator')\n",
    "            plt.legend(loc = 'upper right')\n",
    "            plt.show()\n",
    "\n",
    "        # Save the image and show it\n",
    "        if (i) % save_ratio == 0 and i != 0:\n",
    "            # Print a Scatter plot of the real vs fake values\n",
    "            [eval_labels_real, eval_x_real], eval_y_real = generate_real_samples(x_train, y_train, 32)\n",
    "            [eval_x_fake, eval_labels], eval_y_fake = generate_fake_samples(g_model, latent_dim, 32, eval_labels_real)\n",
    "            print(f'Epoch {i}/{n_epochs}')\n",
    "            plt.scatter(y = eval_x_real, x = range(len(eval_x_real)), color='red', label='Real')\n",
    "            plt.scatter(y = eval_x_fake, x = range(len(eval_x_fake)), color='blue', label='Predicted')\n",
    "            # Add vertical lines at each point\n",
    "            for a in range(len(eval_x_real)):\n",
    "                # Make the line go from the minimum value to the maximum value\n",
    "                min_val = min(eval_x_real[a], eval_x_fake[a])\n",
    "                max_val = max(eval_x_real[a], eval_x_fake[a])\n",
    "                plt.vlines(x=a, ymin=min_val, ymax=max_val, color='black', linestyles='dotted')\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(cwd, company, exp_name, 'images', f'plot_epoch_{i}.png'))\n",
    "            plt.show() \n",
    "            # Save the models\n",
    "            g_model.save(os.path.join(cwd, company, exp_name, 'models', f'{company}_generator_{i}.h5'))\n",
    "            d_model.save(os.path.join(cwd, company, exp_name, 'models', f'{company}_discriminator{i}.h5'))\n",
    "            gan_model.save(os.path.join(cwd, company, exp_name, 'models', f'{company}_gan_{i}.h5'))\n",
    "\n",
    "# Define a function with 3 inputs, the predicted value, the real value and the actual price\n",
    "# With this function i want to know if the price is going to go up or down and if the prediction is correct \n",
    "def evaluate_prediction(predicted_value_array, real_value_array, today_price_array):\n",
    "    \"\"\"\n",
    "    Evaluate if the predicted value correctly indicates the price movement (up or down)\n",
    "    compared to the actual price and the real value.\n",
    "\n",
    "    Parameters:\n",
    "    - predicted_value_array: Array with the predicted values\n",
    "    - real_value_array: Array with the real values\n",
    "    - actual_price_array: Array with the actual prices\n",
    "\n",
    "    Returns:\n",
    "    - correct_predictions: Number of correct predictions\n",
    "    - total_predictions: Total number of predictions\n",
    "    - accuracy: Accuracy of the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the number of correct predictions\n",
    "    correct_predictions = 0\n",
    "    # Initialize the total number of predictions\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Iterate over the predicted values\n",
    "    for i in range(len(predicted_value_array)):\n",
    "        # If the predicted value is greater than the real value\n",
    "        if ((predicted_value_array[i] >= today_price_array[i]) and (real_value_array[i] >= today_price_array[i])) or ((predicted_value_array[i] < today_price_array[i])and (real_value_array[i] < today_price_array[i])):\n",
    "            # Increment the number of correct predictions\n",
    "            correct_predictions += 1\n",
    "            # Increment the total number of predictions\n",
    "        total_predictions += 1\n",
    "    # Calculate the accuracy of the predictions\n",
    "    accuracy = correct_predictions / total_predictions \n",
    "    accuracy = round(100*accuracy, 2)\n",
    "\n",
    "    # Return the number of correct predictions, the total number of predictions and the accuracy\n",
    "    return correct_predictions, total_predictions, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "n_epochs = 151\n",
    "save_ratio = 25\n",
    "\n",
    "n_batch = 32\n",
    "\n",
    "num_features = x_train.shape[2]\n",
    "sequence_length = 20\n",
    "loss = 'binary_crossentropy'\n",
    "opt_d = Adamax(learning_rate=0.0001)\n",
    "opt_gan = Adamax(learning_rate=0.001)\n",
    "last_epoch = 0\n",
    "\n",
    "\n",
    "# Convert the series to a numpy array\n",
    "my_array = np.array(s_and_p_data['Close'])\n",
    "# Calculate the number of rows needed\n",
    "num_rows = len(my_array) - sequence_length - 1\n",
    "# Initialize an empty list to store the rows\n",
    "rows = []\n",
    "\n",
    "# Generate rows of 20 elements each\n",
    "for i in range(num_rows):\n",
    "    rows.append(my_array[i:i+sequence_length])\n",
    "# Convert the list of rows to a numpy array\n",
    "transposed_array = np.array(rows)\n",
    "# Print the transposed array\n",
    "# print(transposed_array)\n",
    "\n",
    "# Experiment name\n",
    "exp_name = 'lstm'\n",
    "if not os.path.exists(os.path.join(cwd, company, exp_name)):\n",
    "    os.makedirs(os.path.join(cwd, company, exp_name))\n",
    "\n",
    "if not os.path.exists(os.path.join(cwd, company, exp_name, 'models')):\n",
    "    os.makedirs(os.path.join(cwd, company, exp_name, 'models'))\n",
    "    \n",
    "# Chek if the generator model already exists in the company folder\n",
    "if os.path.isfile(os.path.join(cwd, company, exp_name, 'models', f'{company}_generator.h5')):\n",
    "    # Load the generator model\n",
    "    generator = load_model(os.path.join(cwd, company, exp_name, 'models', f'{company}_generator.h5'))\n",
    "    # Print the summary of the generator model\n",
    "    generator.summary()\n",
    "else:\n",
    "    # List the content of the company, expereiment and models folders\n",
    "    model_list = os.listdir(os.path.join(cwd, company, exp_name, 'models'))\n",
    "    # If there is at least one model in the list load the ones with the highest epoch number\n",
    "    if len(model_list) > 0:\n",
    "        print('Loading the models with the highest epoch number')\n",
    "        for model in model_list:\n",
    "            if 'generator' in model:\n",
    "                epoch = int(model.split('_')[-1].split('.')[0])\n",
    "                if epoch > last_epoch:\n",
    "                    last_epoch = epoch\n",
    "            # print(f'Last epoch: {last_epoch}')\n",
    "        # Load the generator model with the highest epoch number\n",
    "        generator = load_model(os.path.join(cwd, company, exp_name, 'models', model))\n",
    "        # load the discriminator model with the highest epoch number\n",
    "        discriminator = load_model(os.path.join(cwd, company, exp_name, 'models', f'{company}_discriminator{last_epoch}.h5'))\n",
    "        # load the gan model with the highest epoch number\n",
    "        gan_model = load_model(os.path.join(cwd, company, exp_name, 'models', f'{company}_gan_{last_epoch}.h5'))\n",
    "    else:\n",
    "        last_epoch = 0\n",
    "        # Define the generator model\n",
    "        generator = define_generator(latent_dim, num_features, sequence_length)\n",
    "        # Define the discriminator model\n",
    "        discriminator = define_discriminator(num_features, sequence_length, opt_d, loss)\n",
    "        # Define the GAN model\n",
    "        gan_model = define_gan(generator, discriminator, opt_gan, loss, latent_dim, sequence_length, num_features)\n",
    "    # Train the GAN model\n",
    "    train(generator, discriminator, gan_model, x_train, y_train, latent_dim, exp_name, company, n_epochs, n_batch, save_ratio, last_epoch = last_epoch)\n",
    "    # Save the models\n",
    "    generator.save(os.path.join(cwd, company, exp_name, 'models', f'{company}_generator.h5'))\n",
    "    discriminator.save(os.path.join(cwd, company, exp_name, 'models', f'{company}_discriminator.h5'))\n",
    "    gan_model.save(os.path.join(cwd, company, exp_name, 'models', f'{company}_gan.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
